# 시험 문제 텍스트 유사도 분석 프로젝트 BoW 기반의 벡터화

지난 글에서는 토큰화와 다양한 한국어 형태소를 알아봤습니다. 이번 글에서는 토큰화 한 문서를 벡터화하는 방법 중 하나인 BoW 기반의 단어 카운트 벡터화 방법과 TF-IDF 방법을 알아보겠습니다. 





## BoW 란?

Bag of Words는 토큰을 수치화 하는 방법 중 하나입니다. 문서들의 토큰을 분석하고 해당 토큰을 임의의 인덱스로 지정하여 각각의 문서들을 통계적 기법을 사용해 수치형 배열 구조의 형태로 만들어줍니다. 문장의 토큰을 임의의 인덱스로 지정하면 그 토큰은 하나의 피쳐가 되며, 문서가 많을 수록 피쳐의 크기가 커질 수 있습니다. 크게 WordCount와 TF-IDF 두 가지의 표현 방법이 있습니다.





## WordCountVectorize

문서들에 있는 토큰들을 단순하게 세는 표현 방법입니다. 예를들어 다음과 같이 5개의 토큰화 된 문서가 있다고 가정하겠습니다. 

| 문서                                                      |
| --------------------------------------------------------- |
| '직접 분사식 엔진 의 장점 중 틀린 것 은 ?'                |
| '디젤 기관 의 노킹 방지책 으로 틀린 것 은 ?'              |
| '압력 식 라디에이터 캡 에 대한 설명 으로 적합 한 것 은 ?' |

위에 있는 문장을 WordCountVectorize를 써서 문서의 모든 형태소를 카운트 한다면 다음과 같은 형태가 나올 것 입니다.

| 기관 | 노킹 | 대한 | 디젤 | 라디에이터 | 방지책 | 분사식 | 설명 | 압력 | 엔진 | 으로 | 장점 | 적합 | 직접 | 틀린 |
| ---- | ---- | ---- | ---- | ---------- | ------ | ------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | 0    | 0    | 0    | 0          | 0      | 0.46   | 0    | 0    | 0.46 | 0    | 0.46 | 0    | 0.46 | 0.35 |
| 0.44 | 0.44 | 0    | 0.44 | 0          | 0.44   | 0      | 0    | 0    | 0    | 0.33 | 0    | 0    | 0    | 0.33 |
| 0    | 0    | 0.42 | 0    | 0.42       | 0      | 0      | 0.42 | 0.42 | 0    | 0.32 | 0    | 0.42 | 0    | 0    |

```
# 어휘 사전
{'직접': 12, '분사식': 6, '엔진': 8, '장점': 11, '틀린': 13, '분사': 5, '펌프': 14, '플런저': 15, '배럴': 4, '사이': 7, '윤활': 9, '디젤': 2, '기관': 0, '노킹': 1, '방지책': 3, '으로': 10}

# 문장
'직접 분사식 엔진 의 장점 중 틀린 것 은 ?'
'디젤 기관 의 노킹 방지책 으로 틀린 것 은 ?'
'압력 식 라디에이터 캡 에 대한 설명 으로 적합 한 것 은 ?'
```

단어를 카운팅 하여 벡터를 만들어서 어떠한 문장이 문서 갯수가 많을수록, 단어 집합이 많을 수록 행열의 덩어리가 커지며, 무엇보다 단순 단어 카운팅을 해 문서에서 자주 사용하는 불용어(으로, 것은)도 높게 카운트 합니다. 불용어의 해결을 위해 TF-IDF 라는 표현 방법이 나왔습니다. 





## TF-IDF

WordCountVectorize의 단점을 상쇄시키는 방법입니다. 전체 문서에서 자주 나오는 단어의 중요도는 낮다고 판단하고, 특정 문서에서만 자주 나오는 단어는 높다고 판단합니다. 

TF는 쉽게 말해 CounterVectorize 벡터에서 특정 위치의 값 입니다. IDF는 전체 문서에서 각 문서에 해당 단어가 있는지 없는지에 대한 총 합 입니다. 

* TF-IDF의 공식 = TF * IDF

* TF = 각 문서의 wordCount 값

* IDF = 전체 문서 / 각 문서에서 해당 단어가 있는지 없는지에 대한 총 합(DF)

하지만 문서의 수가 커질 수록 idf 값이 로그를 씌워주어 아무리 증가를 해도 특정 값에 수렴할 수 있게 만들었습니다. 또한 분모에 1을 증가하는 이유는 DF 값이 0이면 계산을 할 수 없기 때문에 1을 증가시킵니다.



![image](https://user-images.githubusercontent.com/71218142/194949209-9238f4e1-67c9-40eb-a969-933d999f450b.png)

참고로 파이썬에서 TfidfVectorizer로 수치화를 할 때 idf를 자연로그로 씌워줍니다. 



예시

현재 TF는 다음과 같습니다. 

| 기관 | 노킹 | 대한 | 디젤 | 라디에이터 | 방지책 | 분사식 | 설명 | 압력 | 엔진 | 으로 | 장점 | 적합 | 직접 | 틀린 |
| ---- | ---- | ---- | ---- | ---------- | ------ | ------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | 0    | 0    | 0    | 0          | 0      | 1      | 0    | 0    | 1    | 0    | 1    | 0    | 1    | 1    |
| 1    | 1    | 0    | 1    | 0          | 1      | 0      | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1    |
| 0    | 0    | 1    | 0    | 1          | 0      | 0      | 1    | 1    | 0    | 1    | 0    | 1    | 0    | 0    |

* 해당 벡터 중 tf("디젤 기관 의 노킹 방지책 으로 틀린 것 은 ?", '노킹')의 값은 1입니다. 

* df("디젤 기관 의 노킹 방지책 으로 틀린 것 은 ?", "노킹")은 전체 문서 갯수에서 노킹이라는 단어가 들어있는 문서는 1개 이므로 1 입니다. 
* 마지막으로 idf의 공식 ln(전체 문서 갯수 / 1 + df)를 하면 TF-IDF 값을 구할 수 있습니다. 

* 예시를 바탕으로 TF(1) * IDF(ln(3/2)) = 약 0.40이 나옵니다.





## TF-IDF 실습을 통한 의문점

* 공식대로 손 연산을 했는데 파이썬에서 사용하는 tfidf으로 환산한 벡터 값이 다름. 
* 아마 단어 수에 따라 차이가 있는걸까?





## 실습 이미지 

**데이터입력**

![image](https://user-images.githubusercontent.com/71218142/194952534-81a133b4-f6b6-45fe-a12d-7de5bedb572d.png)



**CountVectorize**

![image](https://user-images.githubusercontent.com/71218142/194952221-becf665d-44c1-4f63-8a01-36530ba9d3bd.png)





**TfidfVectorizer**

![image](https://user-images.githubusercontent.com/71218142/194952162-1761e898-0a99-4f8e-9cdb-b3d3e6147949.png)







