# 시험 문제 텍스트 유사도 분석 프로젝트 자연어 처리의 전체적인 프로세스

안녕하세요. 저번 글에서는 프로젝트를 위해 데이터를 수집하고, 정제하는 과정을 거쳐 지게차운전기능사 시험 문제 텍스트 문서를 만들었습니다. 이번 글에서는 형태소 토큰화, 텍스트 벡터라이징, 텍스트 유사도의 전체적인 과정을 코드로 간략하게 작성 하겠습니다. 



## 자연어 처리 수행 프로세스 

![image](https://user-images.githubusercontent.com/71218142/194776801-ada3bebd-2be0-43ae-a211-922a3d6f62d3.png)

자연어 처리 수행 프로세스는 크게 4 단계를 거쳐 수행됩니다. 

1. 텍스트 추출 및 클렌징 : 분석 할 자연어를 추출하고 클렌징 합니다. 클렌징의 예는 html의 태그를 제거한다던가 불필요한 텍스트를 삭제한다고 볼 수 있습니다. 지난 글에서 해당 과정을 수행한 것이라 볼 수 있겠네요. 
2. 문서들을 어떠한 기준으로 분리하여 모델 학습을 더 잘 수행할 수 있게 만듭니다. 기준은 단어가 될 수 있고 한글의 경우 형태소가 될 수 있고 문장이 될 수 있습니다. (하지만 문장 토큰화는 잘 쓰이지 않습니다.)  
3. 토큰화를 수행하여 나눠진 문서에 어떠한 기준으로 벡터라이징을 시킵니다. 사람은 자연어를 감각적으로 이해하고 해석하지만 컴퓨터는 자연어를 인식할 수 없습니다. 따라서 벡터화나 배열화를 하여 숫자로 만들어 자연어의 패턴을 익히도록 합니다.
4. 모델링을 기반으로 학습 및 테스트 평가를 수행합니다. 이 모델링은 내가 수행하는 목적에 따라 달라지며 지도 학습, 비지도 학습을 수행토록 합니다.



## 전체적인 프로세스 실습 

1 ~ 4번의 프로세스를 간단한 실습을 통해 파악해보도록 하겠습니다. 어떠한 문장을 적어 해당 문장과 가장 유사도가 높은 문장을 출력해보도록 하겠습니다. 



### 1. 클렌징 한 텍스트 데이터 불러오기 

```python
# txt 파일이 줄글 형태로 된 경우 
f = open('text_corpus.txt', 'r')
data = f.read().splitlines()
```

저번 글에서 추출 및 클렌징한 데이터를 읽고 리스트에 담습니다.

![image](https://user-images.githubusercontent.com/71218142/194777349-2e5cc97f-4737-4702-8d33-07385c1ec4fe.png)

data 리스트 변수에 시험 문제들이 담겨졌네요. 



### 2. 2 ~ 3 프로세스 수행을 위해 필요한 라이브러리 로드 및 처리 

```python
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

okt = Okt()
tfidf_vect = TfidfVectorizer()

def okt_tokenizer(text) : 
    # 입력 인자로 들어온 문장를 형태소로 쪼개어 토큰화하고, 리스트 형태로 반환합니다.
    tokens_ko = okt.morphs(text)
    
    return tokens_ko
```

해당 과정을 수행하기 위해 konlpy(코엔엘파이)모듈 내 Okt와 TfidfVectorizer 라이브러리를 사용했습니다.

konlpy(코엔엘파이)는 한국어를 처리하기 위한 파이썬 패키지 입니다. konlpy 안에는 총 4가지의 한국어 분석기가 있는데 가장 많이 사용되는 분석기는 Okt와 Mecab을 사용한다고 합니다. 

TfidfVectorizer는 토큰화 된 문서를 [tf-idf](https://ko.wikipedia.org/wiki/Tf-idf)로 벡터라이징하여 문서들을 숫자로 만드는 과정을 수행합니다. 토큰화 된 단어를 세는 CounterVector도 있지만 단어 빈도가 높은 것에만 치중하여 tf-idf보다는 효율적이지 못하다고 합니다. 

위 코드의 okt_tokenizer 함수는 임의의 텍스트를 넣으면 형태소를 기준으로 문장을 나눠 리스트로 반환하는 함수입니다. 



```python
doc_morphs_token_lst = []

for text in data : 
    morphs_text_lst = okt_tokenizer(text)
    morphs_text = ' '.join(morphs_text_lst)
    
    doc_morphs_token_lst.append(morphs_text)
    
doc_morphs_token_lst
```

형태소를 기준으로 리스트가 반환 된 값을 morphs_text_lst에 넣고  join을 통해 한 문장이 형태소로 쪼개진 리스트를 문자열로 만듭니다. 이후 doc_morphs_token_lst에 추가합니다. 그러면 doc_morphs_token_lst은 아래의 이미지와 같이 형태소 단위로 띄어쓰기 된 문자열 리스트가 만들어집니다. 

![image](https://user-images.githubusercontent.com/71218142/194777998-88f76bcb-9758-436b-8cea-b7865e4dff75.png)



```python
# tfidf 벡터라이즈
feature_vect = tfidf_vect.fit_transform(doc_morphs_token_lst)
print(feature_vect.shape)

# 1560행 1831열
#feature data가 1831개
```

형태소로 쪼개진 문장이 들어있는 doc_morphs_token_lst을 tf-idf로 벡터화하여 추후 비교하게 될 문장 벡터와 1560개의 문장이 1831개의 의미있는 단어 피쳐를 기준으로 벡터화 되었습니다. 참고로 1,0으로 쭉 늘여놓는 벡터화 기법을 BOW(Bag Of Words)라고 하며, 다른 기법으로는 Word2Vec 기법이 있습니다. TfidfVectorizer()는 BOW 기법을 사용하고 있습니다. 



### 3. 테스트 데이터 토큰화 및 벡터화, 코사인 유사도로 문장 유사도 판별

![image](https://user-images.githubusercontent.com/71218142/194778449-7e7010ad-3682-42e1-bdd2-27e1ed3b1819.png)

```python
new_text = ['라디에이터 캡의 압력스프링 장력이 약화되었을 때 나타나는 현상은?']

doc_morphs_token_lst = []

for text in new_text : 
    morphs_text_lst = okt_tokenizer(text)
    morphs_text = ' '.join(morphs_text_lst)
    
    doc_morphs_token_lst.append(morphs_text)

# 테스트 데이터를 transform 시켜 이전 벡터와 피쳐가 같은 구조인 벡터 구현 
new_post_vec = tfidf_vect.transform(doc_morphs_token_lst)
```



이제 테스트 데이터를 토큰화 및 벡터화 후 코사인 유사도로 이전에 구축한 1560 X 1831 벡터와  비교하겠습니다. 2013년 상시 문제 중 한 문제를 가져왔으며, 테스트 데이터도 이전 과정과 똑같이 구현합니다. 형태소로 토큰화 시키고, tfidf로 벡터화 시키죠. 단 fit_transform 대신 transform을 사용해 이전 벡터와 피쳐가 같은 구조인 벡터를 구현합니다.



```python
'''
코사인 유사도 연산 결과로는 1행 1560열의 행렬을 얻습니다. 
이는 1560개의 각 문서 벡터(시험 문제 데이터)와 자기 자신인 1개의 문서 벡터 간의 유사도가 기록된 행렬입니다. 
모든 1560개 시험의 상호 유사도가 기록되어져 있습니다. 
'''

cosine_sim = cosine_similarity(new_post_vec, feature_vect)
print('코사인 유사도 연산 결과 :',cosine_sim.shape)

# 코사인 유사도 연산 결과 : (1, 1560)
```



이후 코사인 유사도로 테스트 벡터와 훈련 벡터의 유사도를 연산합니다. 참고로 코사인 유사도는 1에 가까울수록 유사도가 높으며, -1이 될수록 유사도가 떨어집니다.  그 결과 shape가 (1, 1560)이 나왔습니다. 

이 shape의 구조는 1560개의 레코드가 있는 벡터와 1개의 테스트 데이터 간 유사도가 기록 된 행렬입니다. 



```python
index_of_max_value = np.argmax(cosine_sim)

print('가장 높은 유사도 인덱스 : ', index_of_max_value,'\n' '가장 높은 유사도 값 : ', np.max(cosine_sim))

'''
가장 높은 유사도 인덱스 :  1263 
가장 높은 유사도 값 :  0.6031886740919079
'''
```

코사인 유사도 연산 결과가 있는 cosine_sim은 np.array 구조로 되어 있습니다.  가장 높은 유사도 인덱스가 1263이며, 이에 대한 유사도 값은 0.603이 나왔습니다. 즉 문서 데이터의 1263번 인덱스에 해당하는 문장이 가장 유사도가 높다는 뜻 입니다. 그럼 1263번의 문장이 어떤건지 한번 볼까요? 



```python
print(f'=========== {new_text[0]} 가장 유사도가 높은 문제 ===========  \n{data[1263]}')


'''
=========== 라디에이터 캡의 압력스프링 장력이 약화되었을 때 나타나는 현상은? 가장 유사도가 높은 문제 ===========  
라디에이터 캡의 스프링이 파손 되었을 때 가장 먼저 나타나는 현상은?
'''
```

테스트 데이터 '라디에이터 캡의 압력스프링 장력이 약화되었을 때 나타나는 현상은?' 문제 중 가장 높은 유사도를 가진 데이터는 '라디에이터 캡의 스프링이 파손 되었을 때 가장 먼저 나타나는 현상은?' 군요! 



이렇듯 수집과 정제 된 말뭉치를 이용해 토큰화하고 벡터화를 통해 컴퓨터가 인지 가능한 데이터로 구성 후 모델을 통해 패턴 놀이를 시켜 자연어 처리를 할 수 있습니다. 다음 글에서는 한글 문서에 대해 유사도 분석에 관련한 토큰화, 벡터화 기법에 대해 작성해보겠습니다.

